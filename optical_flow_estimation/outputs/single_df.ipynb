{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              model checkpoint   attack norm  epsilon  iterations  alpha  \\\n",
      "0              ccmr      kitti     3dcc  NaN      NaN         NaN    NaN   \n",
      "1              ccmr      kitti     3dcc  NaN      NaN         NaN    NaN   \n",
      "2              ccmr      kitti     3dcc  NaN      NaN         NaN    NaN   \n",
      "3              ccmr      kitti     3dcc  NaN      NaN         NaN    NaN   \n",
      "4              ccmr      kitti     3dcc  NaN      NaN         NaN    NaN   \n",
      "...             ...        ...      ...  ...      ...         ...    ...   \n",
      "7318  videoflow_bof     sintel  weather  NaN      NaN         NaN    NaN   \n",
      "7319  videoflow_bof     sintel  weather  NaN      NaN         NaN    NaN   \n",
      "7320  videoflow_bof     sintel  weather  NaN      NaN         NaN    NaN   \n",
      "7321  videoflow_bof     sintel  weather  NaN      NaN         NaN    NaN   \n",
      "7322  videoflow_bof     sintel  weather  NaN      NaN         NaN    NaN   \n",
      "\n",
      "      targeted    target loss  ...   no   lr  unregistered     num  \\\n",
      "0        False       NaN  NaN  ...  NaN  NaN           NaN     NaN   \n",
      "1        False       NaN  NaN  ...  NaN  NaN           NaN     NaN   \n",
      "2        False       NaN  NaN  ...  NaN  NaN           NaN     NaN   \n",
      "3        False       NaN  NaN  ...  NaN  NaN           NaN     NaN   \n",
      "4        False       NaN  NaN  ...  NaN  NaN           NaN     NaN   \n",
      "...        ...       ...  ...  ...  ...  ...           ...     ...   \n",
      "7318      True  negative  epe  ...  0.0  0.0           1.0  3000.0   \n",
      "7319      True      zero  epe  ...  0.0  0.0           1.0     NaN   \n",
      "7320      True      zero  epe  ...  0.0  0.0           1.0    60.0   \n",
      "7321      True      zero  epe  ...  0.0  0.0           1.0  1000.0   \n",
      "7322      True      zero  epe  ...  0.0  0.0           1.0  3000.0   \n",
      "\n",
      "      weather_type           start_time            end_time  duration  \\\n",
      "0              NaN  2024-09-17 08:33:06 2024-09-17 08:40:16  00:07:09   \n",
      "1              NaN  2024-09-17 08:33:09 2024-09-17 08:40:18  00:07:09   \n",
      "2              NaN  2024-09-17 08:33:08 2024-09-17 08:40:21  00:07:12   \n",
      "3              NaN  2024-09-17 09:34:17 2024-09-17 09:41:12  00:06:54   \n",
      "4              NaN  2024-09-17 09:34:02 2024-09-17 09:41:16  00:07:13   \n",
      "...            ...                  ...                 ...       ...   \n",
      "7318        sparks  2024-08-07 15:26:09 2024-08-08 10:35:02  19:08:52   \n",
      "7319          snow  2024-07-13 08:03:48 2024-07-14 01:05:48  17:01:59   \n",
      "7320           fog  2024-07-14 12:36:11 2024-07-15 05:57:30  17:21:19   \n",
      "7321          rain  2024-07-15 19:02:47 2024-07-17 12:51:43  17:48:56   \n",
      "7322        sparks  2024-08-07 15:26:10 2024-08-08 10:38:32  19:12:22   \n",
      "\n",
      "      epe_initial_to_zero  epe_initial_to_negative  \n",
      "0                     NaN                      NaN  \n",
      "1                     NaN                      NaN  \n",
      "2                     NaN                      NaN  \n",
      "3                     NaN                      NaN  \n",
      "4                     NaN                      NaN  \n",
      "...                   ...                      ...  \n",
      "7318                  NaN                      NaN  \n",
      "7319                  NaN                      NaN  \n",
      "7320                  NaN                      NaN  \n",
      "7321                  NaN                      NaN  \n",
      "7322                  NaN                      NaN  \n",
      "\n",
      "[7323 rows x 294 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.0)\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "directory_weather_path = 'validate_weather/'\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "weather_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/[jlx]_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    if model_name == \"maskflownet\" and os.path.basename(os.path.dirname(file_path)).split(\"_\")[1] == \"s\":\n",
    "        model_name = \"maskflownet_s\"\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[2].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        all_experiments.append(experiment_flat)\n",
    "\n",
    "\n",
    "\n",
    "for file_path in glob.glob(os.path.join(directory_weather_path, '**/metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    if model_name == \"maskflownet\" and os.path.basename(os.path.dirname(file_path)).split(\"_\")[1] == \"s\":\n",
    "        model_name = \"maskflownet_s\"\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[2].split(\".\")[0]\n",
    "    weather_type = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        experiment_flat['weather_type'] = weather_type\n",
    "        # Append the processed experiment to the list\n",
    "        #all_experiments.append(experiment_flat)\n",
    "        weather_experiments.append(experiment_flat)\n",
    "\n",
    "\n",
    "df_weather = pd.DataFrame(weather_experiments)\n",
    "df_weather.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "df_weather.drop(\"data\", axis=\"columns\", inplace=True)\n",
    "df_weather['end_time'] = pd.to_datetime(df_weather['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "df_weather = df_weather.sort_values(by='end_time')\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack','loss', 'optimizer', 'learn', 'alph', 'rendering', 'transparency', 'depth', 'scene', 'recolor', 'do', 'motionblur', 'flakesize', 'constant' , 'motion', 'flake', 'frame', 'no', 'lr', 'unregistered', 'num', 'targeted', 'target', 'dataset']\n",
    "df_weather = df_weather.drop_duplicates(subset=unique_columns, keep='last')\n",
    "#df_weather.to_csv(\"test.csv\")\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "df_weather['model'] = df_weather['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "df_weather['model'] = df_weather['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "df_weather['model'] = df_weather['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "df_weather.loc[(df_weather[\"dataset\"] == \"kitti-2015\") & (df_weather[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "df = pd.DataFrame(all_experiments)\n",
    "df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set iterations and epsilon to 0 where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'iterations'] = 0\n",
    "df.loc[df['attack'] == 'none', 'epsilon'] = 0\n",
    "# Replace 'epe_orig_preds' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_orig_preds'] = df['epe']\n",
    "# Replace 'epe_ground_truth' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_ground_truth'] = df['epe']\n",
    "\n",
    "# Get unique attack types excluding 'none'\n",
    "attack_types = df['attack'].unique()\n",
    "attack_types = attack_types[attack_types != 'none']\n",
    "\n",
    "# Filter entries with attack == 'none'\n",
    "none_entries = df[df['attack'] == 'none']\n",
    "\n",
    "# Create copies of 'none' entries for each attack type\n",
    "new_entries = []\n",
    "for attack in attack_types:\n",
    "    temp = none_entries.copy()\n",
    "    temp['attack'] = attack\n",
    "    new_entries.append(temp)\n",
    "\n",
    "# Combine all new entries into a single DataFrame\n",
    "new_entries_df = pd.concat(new_entries)\n",
    "\n",
    "# Combine the new entries with the original DataFrame\n",
    "result_df = pd.concat([df, new_entries_df])\n",
    "\n",
    "df = df.rename(columns={'corruption': '3dcc_corruption', 'intensity': '3dcc_intensity'})\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "df = df.sort_values(by='end_time')\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim', 'name', 'severity', '3dcc_corruption', '3dcc_intensity']\n",
    "df = df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "df['optim'] = df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "df['model'] = df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "df['model'] = df['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "df['model'] = df['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "df.loc[(df[\"dataset\"] == \"kitti-2015\") & (df[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "\n",
    "# Find the missing columns in each DataFrame\n",
    "df_missing_cols = [col for col in df_weather.columns if col not in df.columns]\n",
    "df_weather_missing_cols = [col for col in df.columns if col not in df_weather.columns]\n",
    "\n",
    "# Add missing columns to df and fill them with NaN\n",
    "for col in df_missing_cols:\n",
    "    df[col] = np.nan\n",
    "\n",
    "# Add missing columns to df_weather and fill them with NaN\n",
    "for col in df_weather_missing_cols:\n",
    "    df_weather[col] = np.nan\n",
    "\n",
    "# Reorder columns in both dataframes to match (optional, but helpful for clean appending)\n",
    "df_weather = df_weather[df.columns]  # reorder df columns to match df_weather\n",
    "\n",
    "# Append the two DataFrames\n",
    "df = pd.concat([df, df_weather], ignore_index=True)\n",
    "\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "big_df = df\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/[jlx]_iteration_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    if model_name == \"maskflownet\" and os.path.basename(os.path.dirname(file_path)).split(\"_\")[1] == \"s\":\n",
    "        model_name = \"maskflownet_s\"\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[3].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        experiment_flat_key_rename = {}\n",
    "        for key, value in experiment_flat.items():\n",
    "            if key.startswith(\"epe_ground_truth_\"):\n",
    "                new_key = key.replace(\"epe_ground_truth_\", \"epe_gt_\")\n",
    "                experiment_flat_key_rename[new_key] = value\n",
    "            else:\n",
    "                experiment_flat_key_rename[key] = value\n",
    "        #print(experiment_flat_key_rename)\n",
    "        all_experiments.append(experiment_flat_key_rename)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "iterations_df = pd.DataFrame(all_experiments)\n",
    "iterations_df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "iterations_df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "iterations_df = iterations_df.sort_values(by='end_time')\n",
    "\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim']\n",
    "iterations_df = iterations_df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "\n",
    "# Add i0 to the iterations dataframe\n",
    "none_df = df[df['attack'] == 'none']\n",
    "\n",
    "# Select only the necessary columns for the join\n",
    "none_df = none_df[['model', 'checkpoint', 'dataset', 'epe', \"epe_initial_to_negative\", \"epe_initial_to_zero\"]]\n",
    "none_df.rename(columns={'epe': 'epe_gt_i0'}, inplace=True)\n",
    "\n",
    "iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Perform the join operation to add `epe_i0` to `iterations_df`\n",
    "iterations_df = pd.merge(iterations_df, none_df, on=['model', 'checkpoint', 'dataset'], how='left')\n",
    "\n",
    "iterations_df['epe_target_i0'] = iterations_df.apply(\n",
    "    lambda row: row['epe_initial_to_negative'] if row['target'] == 'negative' else row['epe_initial_to_zero'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Sort the dataframe by 'start_time' to ensure it remains ordered\n",
    "iterations_df = iterations_df.sort_values(by='start_time')\n",
    "\n",
    "iterations_df = iterations_df.dropna(subset=['epe_gt_i20'])\n",
    "# Replace optim=NaN with ground truth\n",
    "iterations_df['optim'] = iterations_df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "iterations_df['model'] = iterations_df['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "iterations_df.loc[(iterations_df[\"dataset\"] == \"kitti-2015\") & (iterations_df[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "# Display the updated dataframe\n",
    "#iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Add missing columns to df1 with NaN values\n",
    "#for col in missing_columns:\n",
    "#    big_df[col] = np.nan\n",
    "# Key columns excluding 'start_time' and 'end_time'\n",
    "key_columns = ['model', 'checkpoint', 'attack', 'norm', 'epsilon',\n",
    "               'targeted', 'target', 'loss', 'dataset',\n",
    "               'iterations', 'alpha',\n",
    "               'optim', 'boxconstraint']\n",
    "\n",
    "# Perform an outer merge on all key columns\n",
    "merged_df = pd.merge(iterations_df, big_df, how='outer', on=key_columns, suffixes=('_iter', '_big'))\n",
    "\n",
    "# Retain only 'start_time' and 'end_time' from 'big_df'\n",
    "# First, rename the 'start_time_big' and 'end_time_big' to 'start_time' and 'end_time'\n",
    "merged_df['start_time'] = merged_df['start_time_big']\n",
    "merged_df['end_time'] = merged_df['end_time_big']\n",
    "merged_df['duration'] = merged_df['duration_big']\n",
    "#merged_df['epe_initial_to_negative'] = merged_df['epe_initital_to_zero']\n",
    "\n",
    "# Drop the other 'start_time' and 'end_time' columns from 'iterations_df' (i.e., '_iter' suffixed columns)\n",
    "merged_df.drop(columns=['start_time_iter', 'end_time_iter', 'start_time_big', 'end_time_big', 'duration_big', 'duration_iter'], inplace=True)\n",
    "\n",
    "# The resulting DataFrame will now have only the 'start_time' and 'end_time' from big_df\n",
    "columns_to_process = ['epe_initial_to_zero', 'epe_initial_to_negative']\n",
    "\n",
    "# Loop through each column and apply the logic\n",
    "for col in columns_to_process:\n",
    "    # Create the new column without suffix\n",
    "    merged_df[col] = merged_df[col + '_big'].combine_first(merged_df[col + '_iter'])\n",
    "    \n",
    "    # Drop the _big and _iter columns now that we've merged them\n",
    "    merged_df.drop(columns=[col + '_big', col + '_iter'], inplace=True)\n",
    "\n",
    "# Now the dataframe contains only the new merged columns without suffixes\n",
    "print(merged_df)\n",
    "def map_dataset_values(value):\n",
    "    if value == 'final':\n",
    "        return 'sintel-final'\n",
    "    elif value == 'clean':\n",
    "        return 'sintel-clean'\n",
    "    elif value == '2015':\n",
    "        return 'kitti-2015'\n",
    "    else:\n",
    "        return value  # Leave the value unchanged if it doesn't match\n",
    "\n",
    "# Apply the function to the 'dataset' column\n",
    "merged_df['dataset'] = merged_df['dataset'].apply(map_dataset_values)\n",
    "\n",
    "merged_df = merged_df.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "merged_df.to_csv(\"one_single.csv\")\n",
    "\n",
    "weather_df = merged_df[merged_df['attack'] == 'weather']\n",
    "\n",
    "# Save the filtered data to a CSV file\n",
    "weather_df.to_csv('weather_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing combinations saved to 'missing_combinations.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combinations_list = [\n",
    "    #Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    # {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    # {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    # {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    # {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    # {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    # {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    # {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    # {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    # {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    # {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    # {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    # {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    # {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    # {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    # {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    # {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    # {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    # {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    # {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    # {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    # {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    # {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    # {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    # {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    # {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    # #Top 9 models configuration\n",
    "    # {'targeted': False, 'attack': 'cospgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "    # {'targeted': False, 'attack': 'pgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "\n",
    "    # # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    # {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    # {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    # {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    {'attack': 'common_corruptions', 'name': 'gaussian_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'shot_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'impulse_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'defocus_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'glass_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'motion_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'zoom_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'snow', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'frost', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'fog', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'brightness', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'contrast', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'elastic_transform', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'pixelate', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'jpeg_compression', 'severity': 3.0},\n",
    "\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'far_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'near_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'fog_3d', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'color_quant', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'iso_noise', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'low_light', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'xy_motion_blur', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'z_motion_blur', '3dcc_intensity': 3.0},\n",
    "\n",
    "    # {'attack': 'weather', 'targeted': False, 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'negative', 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'zero', 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "\n",
    "    # {'attack': 'weather', 'targeted': False, 'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0,  'weather_type': 'rain'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0, 'weather_type': 'rain'},\n",
    "    # {'attack': 'weather', 'targeted': True,'target': 'zero',  'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0, 'weather_type': 'rain'},\n",
    "\n",
    "    # {'attack': 'weather', 'targeted': False, 'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0,  'weather_type': 'fog'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0, 'weather_type': 'fog'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'zero',  'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0, 'weather_type': 'fog'},\n",
    "    \n",
    "    # {'attack': 'weather', 'targeted': False, 'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0,  'weather_type': 'sparks'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0, 'weather_type': 'sparks'},\n",
    "    # {'attack': 'weather', 'targeted': True, 'target': 'zero',  'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0, 'weather_type': 'sparks'},\n",
    "\n",
    "    # {'attack': 'none'},\n",
    "    # {'attack': 'pcfa', 'targeted': True, 'target': 'negative', 'epsilon': 0.05, 'iterations': 20.0, 'alpha': 0.0},\n",
    "    # {'attack': 'pcfa', 'targeted': True, 'target': 'zero', 'epsilon': 0.05, 'iterations': 20.0, 'alpha': 0.0},\n",
    "\n",
    "]\n",
    "\n",
    "# Set the dataset and norm\n",
    "#datasets = [\"sintel-final\", \"sintel-clean\", \"kitti-2015\"]\n",
    "# datasets = [\"kitti-2015\"]\n",
    "datasets = [\"sintel-final\", \"sintel-clean\"]\n",
    "\n",
    "# Define the datasets and corresponding model names\n",
    "kitti_model_names = [\n",
    "    \"raft\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"csflow\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"flow1d\", \"flowformer\", \"flowformer++\", \n",
    "    \"gmflow\", \"gmflownet\", \"hd3\", \"irr_pwc\", \"liteflownet\", \n",
    "    \"liteflownet3_pseudoreg\", \"llaflow\", \"ms_raft+\", \n",
    "    \"rapidflow\", \"scopeflow\", \"skflow\", \n",
    "    \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "sintel_model_names = [\n",
    "    \"raft\", \"pwcnet\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"maskflownet_s\", \"flow1d\", \"flowformer\", \n",
    "    \"flowformer++\", \"gmflow\", \"hd3\", \"liteflownet\", \n",
    "    \"liteflownet2\", \"liteflownet3\", \"llaflow\", \n",
    "    \"ms_raft+\", \"rapidflow\", \"scopeflow\", \n",
    "    \"skflow\", \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "kitti_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3_pseudoreg\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "sintel_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the casting dictionary\n",
    "attributes_to_cast = {\n",
    "    'attack': str,\n",
    "    'targeted': 'boolean',  # Can use a boolean cast, True/False or None\n",
    "    'iterations': float,\n",
    "    'alpha': float,\n",
    "    'loss': str,\n",
    "    'epsilon': float,\n",
    "    'target': str,\n",
    "    'norm': str,\n",
    "    'name': str,\n",
    "    'severity': float\n",
    "}\n",
    "\n",
    "# Function to cast columns in merged_df\n",
    "def cast_columns(merged_df, attributes_to_cast):\n",
    "    for col, dtype in attributes_to_cast.items():\n",
    "        if dtype == 'boolean':\n",
    "            merged_df[col] = merged_df[col].astype('bool', errors='ignore')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].astype(dtype, errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Cast the columns\n",
    "merged_df = cast_columns(merged_df, attributes_to_cast)\n",
    "\n",
    "# Function to match combinations\n",
    "def combination_matches_row(input_df, combination_list, output_df, model, dataset):\n",
    "    # Iterate over each combination in the combination list\n",
    "    for combination in combination_list:\n",
    "        combination_in_input_df = False\n",
    "\n",
    "        # **Check if \"optim\" exists in combination and restrict it to top 9 models**\n",
    "        if \"optim\" in combination.keys() and combination[\"optim\"] == \"initial_flow\":\n",
    "            if dataset == \"kitti-2015\":\n",
    "                if model not in kitti_model_names_top_9:\n",
    "                    continue  # Skip this combination if the model is not in top 9 for kitti-2015\n",
    "            else:\n",
    "                if model not in sintel_model_names_top_9:\n",
    "                    continue  # Skip this combination if the model is not in top 9 for other datasets\n",
    "\n",
    "        # **Check for common corruptions only for kitti-2015 dataset**\n",
    "        #if combination['attack'] == 'common_corruptions' and dataset != 'kitti-2015':\n",
    "            #continue  # Skip common corruptions if not kitti-2015 dataset\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in input_df.iterrows():\n",
    "            row_in_combination = True  # Assume the row matches the combination initially\n",
    "            row_dict = row.to_dict()\n",
    "\n",
    "            # Loop through each key-value pair in the combination\n",
    "            for key, value in combination.items():\n",
    "                if key not in row_dict.keys() or row_dict[key] != value:\n",
    "                    row_in_combination = False\n",
    "                    break  # Exit the inner loop because we know this row doesn't match\n",
    "\n",
    "            # If row_in_combination is still True after the inner loop, it means the row matches the combination\n",
    "            if row_in_combination:\n",
    "                combination_in_input_df = True\n",
    "                break  # No need to continue checking rows, as we found a match\n",
    "\n",
    "        # If no matching row was found, add the combination to output_df\n",
    "        if not combination_in_input_df:\n",
    "            combination[\"model\"] = model\n",
    "            combination[\"dataset\"] = dataset\n",
    "            output_df = pd.concat([output_df, pd.DataFrame([combination])], ignore_index=True)\n",
    "            del combination[\"model\"]\n",
    "            del combination[\"dataset\"]\n",
    "\n",
    "    return output_df\n",
    "\n",
    "# Create an empty dataframe to store missing combinations\n",
    "missing_comb_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset in datasets:\n",
    "    # Select the relevant model names based on the dataset\n",
    "    if \"kitti\" in dataset:\n",
    "        model_names = kitti_model_names\n",
    "    else:\n",
    "        model_names = sintel_model_names\n",
    "    \n",
    "    # Iterate through each model in the dataset\n",
    "    for model in model_names:\n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        subset_df = merged_df[(merged_df['dataset'] == dataset) & (merged_df['model'] == model)]\n",
    "        missing_comb_df = combination_matches_row(subset_df, combinations_list, missing_comb_df, model, dataset)\n",
    "\n",
    "# Rearrange columns to have 'model' and 'dataset' first\n",
    "columns_order = ['model', 'dataset'] + [col for col in missing_comb_df.columns if col not in ['model', 'dataset']]\n",
    "missing_comb_df = missing_comb_df[columns_order]\n",
    "\n",
    "# Sort by 'model', 'dataset', 'norm', and 'attack'\n",
    "if 'norm' in missing_comb_df.columns:\n",
    "    missing_comb_df = missing_comb_df.sort_values(by=['model', 'dataset','norm', 'attack'], ascending=True)\n",
    "else:\n",
    "    missing_comb_df = missing_comb_df.sort_values(by=['model', 'dataset', 'attack'], ascending=True)\n",
    "\n",
    "# Save the missing combinations dataframe to a CSV file\n",
    "missing_comb_df.to_csv('missing_combinations.csv', index=False)\n",
    "\n",
    "print(\"Missing combinations saved to 'missing_combinations.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant combinations saved to 'existing_relevant_combinations.csv'\n"
     ]
    }
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant combinations saved to 'existing_relevant_combinations.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combinations_list = [\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    #Top 9 models configuration\n",
    "    {'targeted': False, 'attack': 'cospgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "    {'targeted': False, 'attack': 'pgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    # PCFA\n",
    "    {'attack': 'pcfa', 'iterations': 20.0, 'epsilon': 0.05, 'target': 'negative'},\n",
    "    {'attack': 'pcfa', 'iterations': 20.0, 'epsilon': 0.05, 'target': 'zero'},\n",
    "\n",
    "    {'attack': 'common_corruptions', 'name': 'gaussian_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'shot_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'impulse_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'defocus_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'glass_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'motion_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'zoom_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'snow', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'frost', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'fog', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'brightness', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'contrast', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'elastic_transform', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'pixelate', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'jpeg_compression', 'severity': 3.0},\n",
    "\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'far_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'near_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'fog_3d', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'color_quant', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'iso_noise', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'low_light', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'xy_motion_blur', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'z_motion_blur', '3dcc_intensity': 3.0},\n",
    "\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'far_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'near_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'fog_3d', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'color_quant', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'iso_noise', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'low_light', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'xy_motion_blur', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'z_motion_blur', '3dcc_intensity': 3.0},\n",
    "\n",
    "    {'attack': 'weather', 'targeted': False, 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative', 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'zero', 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "\n",
    "    {'attack': 'weather', 'targeted': False, 'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0,  'weather_type': 'rain'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0, 'weather_type': 'rain'},\n",
    "    {'attack': 'weather', 'targeted': True,'target': 'zero',  'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0, 'weather_type': 'rain'},\n",
    "\n",
    "    {'attack': 'weather', 'targeted': False, 'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0,  'weather_type': 'fog'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0, 'weather_type': 'fog'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'zero',  'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0, 'weather_type': 'fog'},\n",
    "    \n",
    "    {'attack': 'weather', 'targeted': False, 'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0,  'weather_type': 'sparks'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0, 'weather_type': 'sparks'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'zero',  'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0, 'weather_type': 'sparks'},\n",
    "\n",
    "    {'attack': 'none'},\n",
    "    {'attack': 'pcfa', 'targeted': True, 'target': 'negative', 'epsilon': 0.05, 'iterations': 20.0, 'alpha': 0.0},\n",
    "    {'attack': 'pcfa', 'targeted': True, 'target': 'zero', 'epsilon': 0.05, 'iterations': 20.0, 'alpha': 0.0},\n",
    "\n",
    "]\n",
    "\n",
    "# Set the dataset and norm\n",
    "datasets = [\"sintel-final\", \"sintel-clean\", \"kitti-2015\"]\n",
    "\n",
    "# Define the datasets and corresponding model names\n",
    "kitti_model_names = [\n",
    "    \"raft\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"csflow\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"flow1d\", \"flowformer\", \"flowformer++\", \n",
    "    \"gmflow\", \"gmflownet\", \"hd3\", \"irr_pwc\", \"liteflownet\", \n",
    "    \"liteflownet3_pseudoreg\", \"llaflow\", \"ms_raft+\", \n",
    "    \"rapidflow\", \"scopeflow\", \"skflow\", \n",
    "    \"starflow\", \"videoflow_bof\", \"splatflow\"\n",
    "]\n",
    "\n",
    "sintel_model_names = [\n",
    "    \"raft\", \"pwcnet\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"maskflownet_s\", \"flow1d\", \"flowformer\", \n",
    "    \"flowformer++\", \"gmflow\", \"hd3\", \"liteflownet\", \n",
    "    \"liteflownet2\", \"liteflownet3\", \"llaflow\", \n",
    "    \"ms_raft+\", \"rapidflow\", \"scopeflow\", \n",
    "    \"skflow\", \"starflow\", \"videoflow_bof\", \"neuflow\"\n",
    "]\n",
    "\n",
    "kitti_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3_pseudoreg\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "sintel_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the casting dictionary\n",
    "attributes_to_cast = {\n",
    "    'attack': str,\n",
    "    'targeted': 'boolean',  # Can use a boolean cast, True/False or None\n",
    "    'iterations': float,\n",
    "    'alpha': float,\n",
    "    'loss': str,\n",
    "    'epsilon': float,\n",
    "    'target': str,\n",
    "    'norm': str,\n",
    "    'name': str,\n",
    "    'severity': float\n",
    "}\n",
    "\n",
    "# Function to cast columns in merged_df\n",
    "def cast_columns(merged_df, attributes_to_cast):\n",
    "    for col, dtype in attributes_to_cast.items():\n",
    "        if dtype == 'boolean':\n",
    "            merged_df[col] = merged_df[col].astype('bool', errors='ignore')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].astype(dtype, errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Cast the columns\n",
    "merged_df = cast_columns(merged_df, attributes_to_cast)\n",
    "\n",
    "def filter_and_append_rows_by_combination(input_df, combination_list, output_df):\n",
    "    # Iterate over each combination in the combination list\n",
    "    for combination in combination_list:\n",
    "        # Initialize the mask to True for all rows\n",
    "        combination_mask = pd.Series([True] * len(input_df), index=input_df.index)\n",
    "\n",
    "        # Loop through each key-value pair in the combination\n",
    "        for key, value in combination.items():\n",
    "            if key in input_df.columns:\n",
    "                # Update the mask for the current combination\n",
    "                combination_mask &= (input_df[key] == value)\n",
    "\n",
    "        # Filter the input_df to keep only the matching rows for the current combination\n",
    "        filtered_df = input_df[combination_mask].reset_index(drop=True)\n",
    "\n",
    "        # Concatenate filtered_df with output_df\n",
    "        output_df = pd.concat([output_df, filtered_df], ignore_index=True)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty dataframe to store missing combinations\n",
    "relevant_experiments = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset in datasets:\n",
    "    # Select the relevant model names based on the dataset\n",
    "    if \"kitti\" in dataset:\n",
    "        model_names = kitti_model_names\n",
    "    else:\n",
    "        model_names = sintel_model_names\n",
    "    \n",
    "    # Iterate through each model in the dataset\n",
    "    for model in model_names:\n",
    "        \n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        subset_df = merged_df[(merged_df['dataset'] == dataset) & (merged_df['model'] == model)]\n",
    "\n",
    "        relevant_experiments = filter_and_append_rows_by_combination(subset_df, combinations_list, relevant_experiments)\n",
    "\n",
    "# Rearrange columns to have 'model' and 'dataset' first\n",
    "columns_order = ['model', 'dataset'] + [col for col in relevant_experiments.columns if col not in ['model', 'dataset']]\n",
    "relevant_experiments = relevant_experiments[columns_order]\n",
    "\n",
    "# Sort by 'model', 'dataset', 'norm', and 'attack'\n",
    "relevant_experiments = relevant_experiments.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "with open('model_params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a DataFrame\n",
    "df_params = pd.DataFrame(data['experiment'])\n",
    "\n",
    "final_df = pd.merge(left=relevant_experiments, right=df_params, on=\"model\")\n",
    "\n",
    "# Save the missing combinations dataframe to a CSV file\n",
    "final_df.to_csv('existing_relevant_combinations.csv', index=False)\n",
    "\n",
    "print(\"Relevant combinations saved to 'existing_relevant_combinations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>point_matching_method</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ccmr</td>\n",
       "      <td>10780884.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>01/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>craft</td>\n",
       "      <td>6307435.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csflow</td>\n",
       "      <td>5604672.0</td>\n",
       "      <td>correlation</td>\n",
       "      <td>02/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dicl</td>\n",
       "      <td>11226036.0</td>\n",
       "      <td>cnn</td>\n",
       "      <td>10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dip</td>\n",
       "      <td>5371968.0</td>\n",
       "      <td>correlation</td>\n",
       "      <td>04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fastflownet</td>\n",
       "      <td>1366114.0</td>\n",
       "      <td>cnn</td>\n",
       "      <td>03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flow1d</td>\n",
       "      <td>5734208.0</td>\n",
       "      <td>attention + correlation</td>\n",
       "      <td>04/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flowformer</td>\n",
       "      <td>16168113.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flowformer++</td>\n",
       "      <td>16152338.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>03/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gma</td>\n",
       "      <td>5879873.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>04/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gmflow</td>\n",
       "      <td>4680288.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gmflownet</td>\n",
       "      <td>9343248.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hd3</td>\n",
       "      <td>39561975.0</td>\n",
       "      <td>cnn</td>\n",
       "      <td>12/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>irr_pwc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cnn</td>\n",
       "      <td>04/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>liteflownet</td>\n",
       "      <td>5379613.0</td>\n",
       "      <td>cnn</td>\n",
       "      <td>05/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>liteflownet2</td>\n",
       "      <td>6429120.0</td>\n",
       "      <td>cnn</td>\n",
       "      <td>02/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liteflownet3</td>\n",
       "      <td>7524188.0</td>\n",
       "      <td>cnn</td>\n",
       "      <td>07/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>llaflow</td>\n",
       "      <td>6060355.0</td>\n",
       "      <td>attention + cost volume</td>\n",
       "      <td>04/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>maskflownet_s</td>\n",
       "      <td>10514256.0</td>\n",
       "      <td>attention + correlation</td>\n",
       "      <td>03/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>maskflownet</td>\n",
       "      <td>20655716.0</td>\n",
       "      <td>attention + correlation</td>\n",
       "      <td>03/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ms_raft+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cost volume + correlation</td>\n",
       "      <td>10/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>matchflow</td>\n",
       "      <td>15446225.0</td>\n",
       "      <td>attention</td>\n",
       "      <td>03/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pwcnet</td>\n",
       "      <td>9374274.0</td>\n",
       "      <td>cnn + cost volume</td>\n",
       "      <td>09/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rapidflow</td>\n",
       "      <td>1646016.0</td>\n",
       "      <td>cnn + cost volume</td>\n",
       "      <td>05/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>raft</td>\n",
       "      <td>5257536.0</td>\n",
       "      <td>correlation</td>\n",
       "      <td>03/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rpknet</td>\n",
       "      <td>2846624.0</td>\n",
       "      <td>cnn + cost volume</td>\n",
       "      <td>03/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>scopeflow</td>\n",
       "      <td>6362092.0</td>\n",
       "      <td>cnn + cost volume</td>\n",
       "      <td>02/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>skflow</td>\n",
       "      <td>6273149.0</td>\n",
       "      <td>cnn + cost volume</td>\n",
       "      <td>11/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>starflow</td>\n",
       "      <td>4772256.0</td>\n",
       "      <td>cnn + cost volume</td>\n",
       "      <td>07/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>videoflow_bof</td>\n",
       "      <td>12659389.0</td>\n",
       "      <td>correlation</td>\n",
       "      <td>03/2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  model_parameters      point_matching_method     time\n",
       "0            ccmr        10780884.0                  attention  01/2024\n",
       "1           craft         6307435.0                  attention  03/2022\n",
       "2          csflow         5604672.0                correlation  02/2022\n",
       "3            dicl        11226036.0                        cnn  10/2020\n",
       "4             dip         5371968.0                correlation  04/2022\n",
       "5     fastflownet         1366114.0                        cnn  03/2021\n",
       "6          flow1d         5734208.0    attention + correlation  04/2021\n",
       "7      flowformer        16168113.0                  attention  03/2022\n",
       "8    flowformer++        16152338.0                  attention  03/2023\n",
       "9             gma         5879873.0                  attention  04/2021\n",
       "10         gmflow         4680288.0                  attention  11/2021\n",
       "11      gmflownet         9343248.0                  attention  03/2022\n",
       "12            hd3        39561975.0                        cnn  12/2018\n",
       "13        irr_pwc               NaN                        cnn  04/2019\n",
       "14    liteflownet         5379613.0                        cnn  05/2018\n",
       "15   liteflownet2         6429120.0                        cnn  02/2020\n",
       "16   liteflownet3         7524188.0                        cnn  07/2020\n",
       "17        llaflow         6060355.0    attention + cost volume  04/2023\n",
       "18  maskflownet_s        10514256.0    attention + correlation  03/2023\n",
       "19    maskflownet        20655716.0    attention + correlation  03/2023\n",
       "20       ms_raft+               NaN  cost volume + correlation  10/2022\n",
       "21      matchflow        15446225.0                  attention  03/2023\n",
       "22         pwcnet         9374274.0          cnn + cost volume  09/2017\n",
       "23      rapidflow         1646016.0          cnn + cost volume  05/2024\n",
       "24           raft         5257536.0                correlation  03/2020\n",
       "25         rpknet         2846624.0          cnn + cost volume  03/2024\n",
       "26      scopeflow         6362092.0          cnn + cost volume  02/2020\n",
       "27         skflow         6273149.0          cnn + cost volume  11/2022\n",
       "28       starflow         4772256.0          cnn + cost volume  07/2020\n",
       "29  videoflow_bof        12659389.0                correlation  03/2023"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[merged_df[\"model\"] == \"neuflow\"]\n",
    "df_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
