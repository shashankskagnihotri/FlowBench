{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              model checkpoint  attack norm  epsilon  iterations  alpha  \\\n",
      "0            flow1d      kitti     pgd  inf   0.0500        20.0   0.01   \n",
      "1            flow1d      kitti     pgd  inf   0.0500        20.0   0.01   \n",
      "2            flow1d      kitti  cospgd  inf   0.0500        20.0   0.01   \n",
      "3            flow1d      kitti  cospgd  inf   0.0500        20.0   0.01   \n",
      "4            flow1d      kitti     bim  inf   0.0157        20.0   0.01   \n",
      "...             ...        ...     ...  ...      ...         ...    ...   \n",
      "5432      scopeflow     sintel    fgsm  two   0.2510         NaN    NaN   \n",
      "5433  videoflow_bof     sintel    fgsm  two   0.2510         NaN    NaN   \n",
      "5434    liteflownet     sintel    fgsm  two   0.2510         NaN    NaN   \n",
      "5435         rpknet     sintel    fgsm  two   0.2510         NaN    NaN   \n",
      "5436        llaflow     sintel    fgsm  two   0.2510         NaN    NaN   \n",
      "\n",
      "      targeted    target loss  ... severity  intensity  corruption  val  \\\n",
      "0         True  negative  epe  ...      NaN        NaN         NaN  NaN   \n",
      "1         True      zero  epe  ...      NaN        NaN         NaN  NaN   \n",
      "2         True      zero  epe  ...      NaN        NaN         NaN  NaN   \n",
      "3         True  negative  epe  ...      NaN        NaN         NaN  NaN   \n",
      "4        False       NaN  epe  ...      NaN        NaN         NaN  NaN   \n",
      "...        ...       ...  ...  ...      ...        ...         ...  ...   \n",
      "5432     False       NaN  epe  ...      NaN        NaN         NaN  NaN   \n",
      "5433     False       NaN  epe  ...      NaN        NaN         NaN  NaN   \n",
      "5434     False       NaN  epe  ...      NaN        NaN         NaN  NaN   \n",
      "5435     False       NaN  epe  ...      NaN        NaN         NaN  NaN   \n",
      "5436     False       NaN  epe  ...      NaN        NaN         NaN  NaN   \n",
      "\n",
      "      conf_f1           start_time            end_time  duration  \\\n",
      "0         NaN  2024-07-05 05:05:00 2024-07-05 05:35:55  00:30:54   \n",
      "1         NaN  2024-07-05 05:09:15 2024-07-05 05:40:19  00:31:04   \n",
      "2         NaN  2024-07-05 05:21:09 2024-07-05 05:52:11  00:31:01   \n",
      "3         NaN  2024-07-05 05:11:06 2024-07-05 05:42:17  00:31:11   \n",
      "4         NaN  2024-07-05 05:37:16 2024-07-05 06:07:59  00:30:42   \n",
      "...       ...                  ...                 ...       ...   \n",
      "5432      NaN  2024-09-14 16:05:19 2024-09-14 16:17:58  00:12:38   \n",
      "5433      NaN  2024-09-14 16:19:26 2024-09-14 17:09:38  00:50:11   \n",
      "5434      NaN  2024-09-14 17:11:46 2024-09-14 17:19:06  00:07:19   \n",
      "5435      NaN  2024-09-14 20:41:40 2024-09-14 21:05:23  00:23:43   \n",
      "5436      NaN  2024-09-15 11:10:07 2024-09-15 11:26:57  00:16:50   \n",
      "\n",
      "      epe_initial_to_zero  epe_initial_to_negative  \n",
      "0               31.204989                62.409978  \n",
      "1               31.204989                62.409978  \n",
      "2               31.204989                62.409978  \n",
      "3               31.204989                62.409978  \n",
      "4               31.204989                62.409978  \n",
      "...                   ...                      ...  \n",
      "5432                  NaN                      NaN  \n",
      "5433                  NaN                      NaN  \n",
      "5434                  NaN                      NaN  \n",
      "5435                  NaN                      NaN  \n",
      "5436                  NaN                      NaN  \n",
      "\n",
      "[5437 rows x 273 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.0)\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/[jlx]_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[2].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        all_experiments.append(experiment_flat)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "df = pd.DataFrame(all_experiments)\n",
    "df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Set iterations and epsilon to 0 where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'iterations'] = 0\n",
    "df.loc[df['attack'] == 'none', 'epsilon'] = 0\n",
    "# Replace 'epe_orig_preds' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_orig_preds'] = df['epe']\n",
    "# Replace 'epe_ground_truth' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_ground_truth'] = df['epe']\n",
    "\n",
    "# Get unique attack types excluding 'none'\n",
    "attack_types = df['attack'].unique()\n",
    "attack_types = attack_types[attack_types != 'none']\n",
    "\n",
    "# Filter entries with attack == 'none'\n",
    "none_entries = df[df['attack'] == 'none']\n",
    "\n",
    "# Create copies of 'none' entries for each attack type\n",
    "new_entries = []\n",
    "for attack in attack_types:\n",
    "    temp = none_entries.copy()\n",
    "    temp['attack'] = attack\n",
    "    new_entries.append(temp)\n",
    "\n",
    "# Combine all new entries into a single DataFrame\n",
    "new_entries_df = pd.concat(new_entries)\n",
    "\n",
    "# Combine the new entries with the original DataFrame\n",
    "result_df = pd.concat([df, new_entries_df])\n",
    "\n",
    "\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "df = df.sort_values(by='end_time')\n",
    "\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim', 'name', 'severity']\n",
    "df = df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "df['optim'] = df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "df['model'] = df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "df['model'] = df['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "df['model'] = df['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "df.loc[(df[\"dataset\"] == \"kitti-2015\") & (df[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "big_df = df\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/[jlx]_iteration_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[3].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        experiment_flat_key_rename = {}\n",
    "        for key, value in experiment_flat.items():\n",
    "            if key.startswith(\"epe_ground_truth_\"):\n",
    "                new_key = key.replace(\"epe_ground_truth_\", \"epe_gt_\")\n",
    "                experiment_flat_key_rename[new_key] = value\n",
    "            else:\n",
    "                experiment_flat_key_rename[key] = value\n",
    "        #print(experiment_flat_key_rename)\n",
    "        all_experiments.append(experiment_flat_key_rename)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "iterations_df = pd.DataFrame(all_experiments)\n",
    "iterations_df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "iterations_df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "iterations_df = iterations_df.sort_values(by='end_time')\n",
    "\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim']\n",
    "iterations_df = iterations_df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "\n",
    "# Add i0 to the iterations dataframe\n",
    "none_df = df[df['attack'] == 'none']\n",
    "\n",
    "# Select only the necessary columns for the join\n",
    "none_df = none_df[['model', 'checkpoint', 'dataset', 'epe', \"epe_initial_to_negative\", \"epe_initial_to_zero\"]]\n",
    "none_df.rename(columns={'epe': 'epe_gt_i0'}, inplace=True)\n",
    "\n",
    "iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Perform the join operation to add `epe_i0` to `iterations_df`\n",
    "iterations_df = pd.merge(iterations_df, none_df, on=['model', 'checkpoint', 'dataset'], how='left')\n",
    "\n",
    "iterations_df['epe_target_i0'] = iterations_df.apply(\n",
    "    lambda row: row['epe_initial_to_negative'] if row['target'] == 'negative' else row['epe_initial_to_zero'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Sort the dataframe by 'start_time' to ensure it remains ordered\n",
    "iterations_df = iterations_df.sort_values(by='start_time')\n",
    "\n",
    "iterations_df = iterations_df.dropna(subset=['epe_gt_i20'])\n",
    "# Replace optim=NaN with ground truth\n",
    "iterations_df['optim'] = iterations_df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "iterations_df['model'] = iterations_df['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "iterations_df.loc[(iterations_df[\"dataset\"] == \"kitti-2015\") & (iterations_df[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "# Display the updated dataframe\n",
    "#iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Add missing columns to df1 with NaN values\n",
    "#for col in missing_columns:\n",
    "#    big_df[col] = np.nan\n",
    "# Key columns excluding 'start_time' and 'end_time'\n",
    "key_columns = ['model', 'checkpoint', 'attack', 'norm', 'epsilon',\n",
    "               'targeted', 'target', 'loss', 'dataset',\n",
    "               'iterations', 'alpha',\n",
    "               'optim', 'boxconstraint']\n",
    "\n",
    "# Perform an outer merge on all key columns\n",
    "merged_df = pd.merge(iterations_df, big_df, how='outer', on=key_columns, suffixes=('_iter', '_big'))\n",
    "\n",
    "# Retain only 'start_time' and 'end_time' from 'big_df'\n",
    "# First, rename the 'start_time_big' and 'end_time_big' to 'start_time' and 'end_time'\n",
    "merged_df['start_time'] = merged_df['start_time_big']\n",
    "merged_df['end_time'] = merged_df['end_time_big']\n",
    "merged_df['duration'] = merged_df['duration_big']\n",
    "#merged_df['epe_initial_to_negative'] = merged_df['epe_initital_to_zero']\n",
    "\n",
    "# Drop the other 'start_time' and 'end_time' columns from 'iterations_df' (i.e., '_iter' suffixed columns)\n",
    "merged_df.drop(columns=['start_time_iter', 'end_time_iter', 'start_time_big', 'end_time_big', 'duration_big', 'duration_iter'], inplace=True)\n",
    "\n",
    "# The resulting DataFrame will now have only the 'start_time' and 'end_time' from big_df\n",
    "columns_to_process = ['epe_initial_to_zero', 'epe_initial_to_negative']\n",
    "\n",
    "# Loop through each column and apply the logic\n",
    "for col in columns_to_process:\n",
    "    # Create the new column without suffix\n",
    "    merged_df[col] = merged_df[col + '_big'].combine_first(merged_df[col + '_iter'])\n",
    "    \n",
    "    # Drop the _big and _iter columns now that we've merged them\n",
    "    merged_df.drop(columns=[col + '_big', col + '_iter'], inplace=True)\n",
    "\n",
    "# Now the dataframe contains only the new merged columns without suffixes\n",
    "print(merged_df)\n",
    "merged_df = merged_df.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "merged_df.to_csv(\"one_single.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing combinations saved to 'missing_combinations.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combinations_list = [\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    #Top 9 models configuration\n",
    "    {'targeted': False, 'attack': 'cospgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "    {'targeted': False, 'attack': 'pgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    {'attack': 'common_corruptions', 'name': 'gaussian_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'shot_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'impulse_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'defocus_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'glass_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'motion_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'zoom_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'snow', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'frost', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'fog', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'brightness', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'contrast', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'elastic_transform', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'pixelate', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'jpeg_compression', 'severity': 3.0},\n",
    "\n",
    "    {'attack': 'none'},\n",
    "\n",
    "]\n",
    "\n",
    "# Set the dataset and norm\n",
    "datasets = [\"sintel-final\", \"sintel-clean\", \"kitti-2015\"]\n",
    "\n",
    "# Define the datasets and corresponding model names\n",
    "kitti_model_names = [\n",
    "    \"raft\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"csflow\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"flow1d\", \"flowformer\", \"flowformer++\", \n",
    "    \"gmflow\", \"gmflownet\", \"hd3\", \"irr_pwc\", \"liteflownet\", \n",
    "    \"liteflownet3_pseudoreg\", \"llaflow\", \"ms_raft+\", \n",
    "    \"rapidflow\", \"scopeflow\", \"skflow\", \n",
    "    \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "sintel_model_names = [\n",
    "    \"raft\", \"pwcnet\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"maskflownet_s\", \"flow1d\", \"flowformer\", \n",
    "    \"flowformer++\", \"gmflow\", \"hd3\", \"liteflownet\", \n",
    "    \"liteflownet2\", \"liteflownet3\", \"llaflow\", \n",
    "    \"ms_raft+\", \"rapidflow\", \"scopeflow\", \n",
    "    \"skflow\", \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "kitti_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3_pseudoreg\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "sintel_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the casting dictionary\n",
    "attributes_to_cast = {\n",
    "    'attack': str,\n",
    "    'targeted': 'boolean',  # Can use a boolean cast, True/False or None\n",
    "    'iterations': float,\n",
    "    'alpha': float,\n",
    "    'loss': str,\n",
    "    'epsilon': float,\n",
    "    'target': str,\n",
    "    'norm': str,\n",
    "    'name': str,\n",
    "    'severity': float\n",
    "}\n",
    "\n",
    "# Function to cast columns in merged_df\n",
    "def cast_columns(merged_df, attributes_to_cast):\n",
    "    for col, dtype in attributes_to_cast.items():\n",
    "        if dtype == 'boolean':\n",
    "            merged_df[col] = merged_df[col].astype('bool', errors='ignore')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].astype(dtype, errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Cast the columns\n",
    "merged_df = cast_columns(merged_df, attributes_to_cast)\n",
    "\n",
    "# Function to match combinations\n",
    "def combination_matches_row(input_df, combination_list, output_df, model, dataset):\n",
    "    # Iterate over each combination in the combination list\n",
    "    for combination in combination_list:\n",
    "        combination_in_input_df = False\n",
    "\n",
    "        # **Check if \"optim\" exists in combination and restrict it to top 9 models**\n",
    "        if \"optim\" in combination.keys() and combination[\"optim\"] == \"initial_flow\":\n",
    "            if dataset == \"kitti-2015\":\n",
    "                if model not in kitti_model_names_top_9:\n",
    "                    continue  # Skip this combination if the model is not in top 9 for kitti-2015\n",
    "            else:\n",
    "                if model not in sintel_model_names_top_9:\n",
    "                    continue  # Skip this combination if the model is not in top 9 for other datasets\n",
    "\n",
    "        # **Check for common corruptions only for kitti-2015 dataset**\n",
    "        #if combination['attack'] == 'common_corruptions' and dataset != 'kitti-2015':\n",
    "            #continue  # Skip common corruptions if not kitti-2015 dataset\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in input_df.iterrows():\n",
    "            row_in_combination = True  # Assume the row matches the combination initially\n",
    "            row_dict = row.to_dict()\n",
    "\n",
    "            # Loop through each key-value pair in the combination\n",
    "            for key, value in combination.items():\n",
    "                if key not in row_dict.keys() or row_dict[key] != value:\n",
    "                    row_in_combination = False\n",
    "                    break  # Exit the inner loop because we know this row doesn't match\n",
    "\n",
    "            # If row_in_combination is still True after the inner loop, it means the row matches the combination\n",
    "            if row_in_combination:\n",
    "                combination_in_input_df = True\n",
    "                break  # No need to continue checking rows, as we found a match\n",
    "\n",
    "        # If no matching row was found, add the combination to output_df\n",
    "        if not combination_in_input_df:\n",
    "            combination[\"model\"] = model\n",
    "            combination[\"dataset\"] = dataset\n",
    "            output_df = pd.concat([output_df, pd.DataFrame([combination])], ignore_index=True)\n",
    "            del combination[\"model\"]\n",
    "            del combination[\"dataset\"]\n",
    "\n",
    "    return output_df\n",
    "\n",
    "# Create an empty dataframe to store missing combinations\n",
    "missing_comb_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset in datasets:\n",
    "    # Select the relevant model names based on the dataset\n",
    "    if \"kitti\" in dataset:\n",
    "        model_names = kitti_model_names\n",
    "    else:\n",
    "        model_names = sintel_model_names\n",
    "    \n",
    "    # Iterate through each model in the dataset\n",
    "    for model in model_names:\n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        subset_df = merged_df[(merged_df['dataset'] == dataset) & (merged_df['model'] == model)]\n",
    "        missing_comb_df = combination_matches_row(subset_df, combinations_list, missing_comb_df, model, dataset)\n",
    "\n",
    "# Rearrange columns to have 'model' and 'dataset' first\n",
    "columns_order = ['model', 'dataset'] + [col for col in missing_comb_df.columns if col not in ['model', 'dataset']]\n",
    "missing_comb_df = missing_comb_df[columns_order]\n",
    "\n",
    "# Sort by 'model', 'dataset', 'norm', and 'attack'\n",
    "missing_comb_df = missing_comb_df.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "# Save the missing combinations dataframe to a CSV file\n",
    "missing_comb_df.to_csv('missing_combinations.csv', index=False)\n",
    "\n",
    "print(\"Missing combinations saved to 'missing_combinations.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant combinations saved to 'existing_relevant_combinations.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combinations_list = [\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    #Top 9 models configuration\n",
    "    {'targeted': False, 'attack': 'cospgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "    {'targeted': False, 'attack': 'pgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    {'attack': 'common_corruptions', 'name': 'gaussian_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'shot_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'impulse_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'defocus_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'glass_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'motion_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'zoom_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'snow', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'frost', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'fog', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'brightness', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'contrast', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'elastic_transform', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'pixelate', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'jpeg_compression', 'severity': 3.0},\n",
    "\n",
    "    {'attack': 'none'},\n",
    "\n",
    "]\n",
    "\n",
    "# Set the dataset and norm\n",
    "datasets = [\"sintel-final\", \"sintel-clean\", \"kitti-2015\"]\n",
    "\n",
    "# Define the datasets and corresponding model names\n",
    "kitti_model_names = [\n",
    "    \"raft\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"csflow\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"flow1d\", \"flowformer\", \"flowformer++\", \n",
    "    \"gmflow\", \"gmflownet\", \"hd3\", \"irr_pwc\", \"liteflownet\", \n",
    "    \"liteflownet3_pseudoreg\", \"llaflow\", \"ms_raft+\", \n",
    "    \"rapidflow\", \"scopeflow\", \"skflow\", \n",
    "    \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "sintel_model_names = [\n",
    "    \"raft\", \"pwcnet\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"maskflownet_s\", \"flow1d\", \"flowformer\", \n",
    "    \"flowformer++\", \"gmflow\", \"hd3\", \"liteflownet\", \n",
    "    \"liteflownet2\", \"liteflownet3\", \"llaflow\", \n",
    "    \"ms_raft+\", \"rapidflow\", \"scopeflow\", \n",
    "    \"skflow\", \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "kitti_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3_pseudoreg\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "sintel_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the casting dictionary\n",
    "attributes_to_cast = {\n",
    "    'attack': str,\n",
    "    'targeted': 'boolean',  # Can use a boolean cast, True/False or None\n",
    "    'iterations': float,\n",
    "    'alpha': float,\n",
    "    'loss': str,\n",
    "    'epsilon': float,\n",
    "    'target': str,\n",
    "    'norm': str,\n",
    "    'name': str,\n",
    "    'severity': float\n",
    "}\n",
    "\n",
    "# Function to cast columns in merged_df\n",
    "def cast_columns(merged_df, attributes_to_cast):\n",
    "    for col, dtype in attributes_to_cast.items():\n",
    "        if dtype == 'boolean':\n",
    "            merged_df[col] = merged_df[col].astype('bool', errors='ignore')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].astype(dtype, errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Cast the columns\n",
    "merged_df = cast_columns(merged_df, attributes_to_cast)\n",
    "\n",
    "def filter_and_append_rows_by_combination(input_df, combination_list, output_df):\n",
    "    # Ensure matching_rows_mask is the same length as the input_df\n",
    "    matching_rows_mask = pd.Series([False] * len(input_df), index=input_df.index)\n",
    "\n",
    "    # Iterate over each combination in the combination list\n",
    "    for combination in combination_list:\n",
    "        # Create a mask to identify rows that match the current combination\n",
    "        combination_mask = pd.Series([True] * len(input_df), index=input_df.index)\n",
    "\n",
    "        # Loop through each key-value pair in the combination\n",
    "        for key, value in combination.items():\n",
    "            if key in input_df.columns:\n",
    "                # Update the mask for the current combination\n",
    "                combination_mask &= (input_df[key] == value)\n",
    "            else:\n",
    "                # If a key in the combination is not in the input_df, skip this combination\n",
    "                combination_mask &= False\n",
    "\n",
    "        # Update the overall matching rows mask\n",
    "        matching_rows_mask |= combination_mask\n",
    "\n",
    "    # Filter the input_df to keep only the matching rows\n",
    "    filtered_df = input_df[matching_rows_mask].reset_index(drop=True)\n",
    "\n",
    "    # Concatenate filtered_df with output_df\n",
    "    output_df = pd.concat([output_df, filtered_df], ignore_index=True)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty dataframe to store missing combinations\n",
    "relevant_experiments = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset in datasets:\n",
    "    # Select the relevant model names based on the dataset\n",
    "    if \"kitti\" in dataset:\n",
    "        model_names = kitti_model_names\n",
    "    else:\n",
    "        model_names = sintel_model_names\n",
    "    \n",
    "    # Iterate through each model in the dataset\n",
    "    for model in model_names:\n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        subset_df = merged_df[(merged_df['dataset'] == dataset) & (merged_df['model'] == model)]\n",
    "        relevant_experiments = filter_and_append_rows_by_combination(subset_df, combinations_list, relevant_experiments)\n",
    "\n",
    "# Rearrange columns to have 'model' and 'dataset' first\n",
    "columns_order = ['model', 'dataset'] + [col for col in relevant_experiments.columns if col not in ['model', 'dataset']]\n",
    "relevant_experiments = relevant_experiments[columns_order]\n",
    "\n",
    "# Sort by 'model', 'dataset', 'norm', and 'attack'\n",
    "relevant_experiments = relevant_experiments.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "# Save the missing combinations dataframe to a CSV file\n",
    "relevant_experiments.to_csv('existing_relevant_combinations.csv', index=False)\n",
    "\n",
    "print(\"Relevant combinations saved to 'existing_relevant_combinations.csv'\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
