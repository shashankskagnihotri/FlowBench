{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         model checkpoint attack norm  epsilon  iterations   alpha  targeted  \\\n",
      "0         ccmr      kitti    bim  inf   0.0157        20.0  0.0100     False   \n",
      "1         ccmr      kitti    bim  inf   0.0314        20.0  0.0100     False   \n",
      "2         ccmr      kitti    bim  inf   0.0500        20.0  0.0100      True   \n",
      "3         ccmr      kitti    bim  inf   0.0500        20.0  0.0100      True   \n",
      "4         ccmr      kitti    bim  two   0.0500        20.0  0.0001     False   \n",
      "...        ...        ...    ...  ...      ...         ...     ...       ...   \n",
      "3175  starflow     sintel    pgd  inf   0.0314        20.0  0.0100     False   \n",
      "3176  starflow     sintel    pgd  inf   0.0314        20.0  0.0100      True   \n",
      "3177  starflow     sintel    pgd  inf   0.0314        20.0  0.0100      True   \n",
      "3178  starflow     sintel    pgd  inf   0.0314        20.0  0.0100      True   \n",
      "3179  starflow     sintel    pgd  inf   0.0314        20.0  0.0100      True   \n",
      "\n",
      "        target loss  ... epe_ground_truth_to_zero  conf_f1  val  name  \\\n",
      "0          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "1          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "2     negative  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "3         zero  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "4          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "...        ...  ...  ...                      ...      ...  ...   ...   \n",
      "3175       NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "3176  negative  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "3177  negative  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "3178      zero  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "3179      zero  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "\n",
      "      severity           start_time            end_time  duration  \\\n",
      "0          NaN  2024-07-09 08:03:21 2024-07-09 09:44:31  01:41:10   \n",
      "1          NaN  2024-07-09 12:55:28 2024-07-09 14:38:33  01:43:04   \n",
      "2          NaN  2024-07-09 05:21:23 2024-07-09 07:02:43  01:41:20   \n",
      "3          NaN  2024-07-09 05:23:23 2024-07-09 07:03:21  01:39:57   \n",
      "4          NaN  2024-07-09 23:37:14 2024-07-10 01:19:03  01:41:48   \n",
      "...        ...                  ...                 ...       ...   \n",
      "3175       NaN  2024-07-25 19:22:51 2024-07-25 20:27:19  01:04:28   \n",
      "3176       NaN  2024-07-23 23:11:36 2024-07-24 00:17:42  01:06:05   \n",
      "3177       NaN  2024-07-25 17:05:45 2024-07-25 18:11:54  01:06:09   \n",
      "3178       NaN  2024-07-24 00:08:46 2024-07-24 01:15:31  01:06:45   \n",
      "3179       NaN  2024-07-25 17:05:45 2024-07-25 18:13:34  01:07:48   \n",
      "\n",
      "      epe_initial_to_zero  epe_initial_to_negative  \n",
      "0               31.192882                62.385763  \n",
      "1               31.192882                62.385763  \n",
      "2               31.192882                62.385763  \n",
      "3               31.192882                62.385763  \n",
      "4               31.192882                62.385763  \n",
      "...                   ...                      ...  \n",
      "3175            12.878002                25.756003  \n",
      "3176            12.858232                25.716464  \n",
      "3177            12.878002                25.756003  \n",
      "3178            12.858232                25.716464  \n",
      "3179            12.878002                25.756003  \n",
      "\n",
      "[3180 rows x 291 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.0)\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[1].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        all_experiments.append(experiment_flat)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "df = pd.DataFrame(all_experiments)\n",
    "df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Set iterations and epsilon to 0 where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'iterations'] = 0\n",
    "df.loc[df['attack'] == 'none', 'epsilon'] = 0\n",
    "# Replace 'epe_orig_preds' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_orig_preds'] = df['epe']\n",
    "# Replace 'epe_ground_truth' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_ground_truth'] = df['epe']\n",
    "\n",
    "# Get unique attack types excluding 'none'\n",
    "attack_types = df['attack'].unique()\n",
    "attack_types = attack_types[attack_types != 'none']\n",
    "\n",
    "# Filter entries with attack == 'none'\n",
    "none_entries = df[df['attack'] == 'none']\n",
    "\n",
    "# Create copies of 'none' entries for each attack type\n",
    "new_entries = []\n",
    "for attack in attack_types:\n",
    "    temp = none_entries.copy()\n",
    "    temp['attack'] = attack\n",
    "    new_entries.append(temp)\n",
    "\n",
    "# Combine all new entries into a single DataFrame\n",
    "new_entries_df = pd.concat(new_entries)\n",
    "\n",
    "# Combine the new entries with the original DataFrame\n",
    "result_df = pd.concat([df, new_entries_df])\n",
    "\n",
    "\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "df = df.sort_values(by='end_time')\n",
    "\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim']\n",
    "df = df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "df['optim'] = df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "df['model'] = df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "big_df = df\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/iteration_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[2].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        all_experiments.append(experiment_flat)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "iterations_df = pd.DataFrame(all_experiments)\n",
    "iterations_df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "iterations_df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "iterations_df = iterations_df.sort_values(by='end_time')\n",
    "\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim']\n",
    "iterations_df = iterations_df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "\n",
    "# Add i0 to the iterations dataframe\n",
    "none_df = df[df['attack'] == 'none']\n",
    "\n",
    "# Select only the necessary columns for the join\n",
    "none_df = none_df[['model', 'checkpoint', 'dataset', 'epe', \"epe_initial_to_negative\", \"epe_initial_to_zero\"]]\n",
    "none_df.rename(columns={'epe': 'epe_gt_i0'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Perform the join operation to add `epe_i0` to `iterations_df`\n",
    "iterations_df = pd.merge(iterations_df, none_df, on=['model', 'checkpoint', 'dataset'], how='left')\n",
    "\n",
    "iterations_df['epe_target_i0'] = iterations_df.apply(\n",
    "    lambda row: row['epe_initial_to_negative'] if row['target'] == 'negative' else row['epe_initial_to_zero'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sort the dataframe by 'start_time' to ensure it remains ordered\n",
    "iterations_df = iterations_df.sort_values(by='start_time')\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "iterations_df['optim'] = iterations_df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "# Display the updated dataframe\n",
    "#iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Add missing columns to df1 with NaN values\n",
    "#for col in missing_columns:\n",
    "#    big_df[col] = np.nan\n",
    "# Key columns excluding 'start_time' and 'end_time'\n",
    "key_columns = ['model', 'checkpoint', 'attack', 'norm', 'epsilon',\n",
    "               'targeted', 'target', 'loss', 'dataset',\n",
    "               'iterations', 'alpha',\n",
    "               'optim', 'boxconstraint']\n",
    "\n",
    "# Perform an outer merge on all key columns\n",
    "merged_df = pd.merge(iterations_df, big_df, how='outer', on=key_columns, suffixes=('_iter', '_big'))\n",
    "\n",
    "# Retain only 'start_time' and 'end_time' from 'big_df'\n",
    "# First, rename the 'start_time_big' and 'end_time_big' to 'start_time' and 'end_time'\n",
    "merged_df['start_time'] = merged_df['start_time_big']\n",
    "merged_df['end_time'] = merged_df['end_time_big']\n",
    "merged_df['duration'] = merged_df['duration_big']\n",
    "#merged_df['epe_initial_to_negative'] = merged_df['epe_initital_to_zero']\n",
    "\n",
    "# Drop the other 'start_time' and 'end_time' columns from 'iterations_df' (i.e., '_iter' suffixed columns)\n",
    "merged_df.drop(columns=['start_time_iter', 'end_time_iter', 'start_time_big', 'end_time_big', 'duration_big', 'duration_iter'], inplace=True)\n",
    "\n",
    "# The resulting DataFrame will now have only the 'start_time' and 'end_time' from big_df\n",
    "columns_to_process = ['epe_initial_to_zero', 'epe_initial_to_negative']\n",
    "\n",
    "# Loop through each column and apply the logic\n",
    "for col in columns_to_process:\n",
    "    # Create the new column without suffix\n",
    "    merged_df[col] = merged_df[col + '_big'].combine_first(merged_df[col + '_iter'])\n",
    "    \n",
    "    # Drop the _big and _iter columns now that we've merged them\n",
    "    merged_df.drop(columns=[col + '_big', col + '_iter'], inplace=True)\n",
    "\n",
    "# Now the dataframe contains only the new merged columns without suffixes\n",
    "print(merged_df)\n",
    "\n",
    "merged_df.to_csv(\"one_single.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate key combinations found.\n",
      "Row count matches between merged_df and big_df.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Key columns for checking duplicates\n",
    "key_columns = ['model', 'checkpoint', 'attack', 'norm', 'epsilon',\n",
    "               'targeted', 'target', 'loss', 'dataset',\n",
    "               'iterations', 'alpha', 'optim', 'boxconstraint']\n",
    "\n",
    "# Step 1: Check for duplicate key combinations in merged_df\n",
    "duplicates = merged_df.groupby(key_columns).size()\n",
    "duplicate_rows = duplicates[duplicates > 1]\n",
    "\n",
    "# Step 2: Check if the number of rows in merged_df matches the number of rows in big_df\n",
    "row_count_matches = len(merged_df) == len(big_df)\n",
    "\n",
    "# Output results\n",
    "if not duplicate_rows.empty:\n",
    "    print(f\"Duplicate key combinations found:\\n{duplicate_rows}\")\n",
    "else:\n",
    "    print(\"No duplicate key combinations found.\")\n",
    "\n",
    "if row_count_matches:\n",
    "    print(\"Row count matches between merged_df and big_df.\")\n",
    "else:\n",
    "    print(f\"Row count mismatch: merged_df has {len(merged_df)} rows, while big_df has {len(big_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         model checkpoint  attack norm  epsilon  iterations  alpha  targeted  \\\n",
      "0         ccmr      kitti     bim  inf   0.0157        20.0   0.01     False   \n",
      "1         ccmr      kitti     bim  inf   0.0314        20.0   0.01     False   \n",
      "2         ccmr      kitti  cospgd  inf   0.0157        20.0   0.01     False   \n",
      "3         ccmr      kitti  cospgd  inf   0.0314        20.0   0.01     False   \n",
      "4         ccmr      kitti    fgsm  inf   0.0157         NaN    NaN     False   \n",
      "...        ...        ...     ...  ...      ...         ...    ...       ...   \n",
      "1009  starflow     sintel     pgd  inf   0.0314        20.0   0.01     False   \n",
      "1010  starflow     sintel     pgd  inf   0.0314        20.0   0.01      True   \n",
      "1011  starflow     sintel     pgd  inf   0.0314        20.0   0.01      True   \n",
      "1012  starflow     sintel     pgd  inf   0.0314        20.0   0.01      True   \n",
      "1013  starflow     sintel     pgd  inf   0.0314        20.0   0.01      True   \n",
      "\n",
      "        target loss  ... epe_ground_truth_to_zero  conf_f1  val  name  \\\n",
      "0          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "1          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "2          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "3          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "4          NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "...        ...  ...  ...                      ...      ...  ...   ...   \n",
      "1009       NaN  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "1010  negative  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "1011  negative  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "1012      zero  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "1013      zero  epe  ...                      NaN      NaN  NaN   NaN   \n",
      "\n",
      "      severity           start_time            end_time  duration  \\\n",
      "0          NaN  2024-07-09 08:03:21 2024-07-09 09:44:31  01:41:10   \n",
      "1          NaN  2024-07-09 12:55:28 2024-07-09 14:38:33  01:43:04   \n",
      "2          NaN  2024-07-09 10:30:32 2024-07-09 12:13:02  01:42:30   \n",
      "3          NaN  2024-07-09 20:04:43 2024-07-09 21:47:32  01:42:49   \n",
      "4          NaN  2024-07-04 21:27:35 2024-07-04 21:39:15  00:11:39   \n",
      "...        ...                  ...                 ...       ...   \n",
      "1009       NaN  2024-07-25 19:22:51 2024-07-25 20:27:19  01:04:28   \n",
      "1010       NaN  2024-07-23 23:11:36 2024-07-24 00:17:42  01:06:05   \n",
      "1011       NaN  2024-07-25 17:05:45 2024-07-25 18:11:54  01:06:09   \n",
      "1012       NaN  2024-07-24 00:08:46 2024-07-24 01:15:31  01:06:45   \n",
      "1013       NaN  2024-07-25 17:05:45 2024-07-25 18:13:34  01:07:48   \n",
      "\n",
      "      epe_initial_to_zero  epe_initial_to_negative  \n",
      "0               31.192882                62.385763  \n",
      "1               31.192882                62.385763  \n",
      "2               31.192882                62.385763  \n",
      "3               31.192882                62.385763  \n",
      "4                     NaN                      NaN  \n",
      "...                   ...                      ...  \n",
      "1009            12.878002                25.756003  \n",
      "1010            12.858232                25.716464  \n",
      "1011            12.878002                25.756003  \n",
      "1012            12.858232                25.716464  \n",
      "1013            12.878002                25.756003  \n",
      "\n",
      "[1014 rows x 291 columns]\n"
     ]
    }
   ],
   "source": [
    "combinations_list = [\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': None, 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'target': None, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': None, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'target': None, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': None, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'target': None, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': None, 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': None, 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': None, 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'target': None, 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'target': None, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'target': None, 'norm': 'two'}\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Create a DataFrame from the combinations_list for easy comparison\n",
    "combinations_df = pd.DataFrame(combinations_list)\n",
    "\n",
    "# Now we want to match specific columns from merged_df with combinations_df\n",
    "# The columns to match are the ones that exist in combinations_list (excluding rows not found in the combinations)\n",
    "\n",
    "# Define the columns to match (these are the keys used in your combinations_list)\n",
    "columns_to_match = ['attack', 'targeted', 'iterations', 'alpha', 'loss', 'epsilon', 'target', 'norm']\n",
    "\n",
    "# Fill missing columns with None or NaN in merged_df to avoid KeyErrors during comparison\n",
    "for col in columns_to_match:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = None\n",
    "\n",
    "# Filter merged_df by checking if its rows exist in combinations_df\n",
    "filtered_df = merged_df.merge(combinations_df, on=columns_to_match, how='inner')\n",
    "\n",
    "# filtered_df now contains only rows from merged_df that match combinations_list\n",
    "print(filtered_df)\n",
    "\n",
    "filtered_df.to_csv(\"filtered_single_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
